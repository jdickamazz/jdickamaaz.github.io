Michael Saxon is an AI researcher specializing in the evaluation of language models and development of multimodal (language & vision) AI systems. He seeks to answer these questions: how can we confidently know what language models are capable of? How can we pre-emptively know which settings they are safe to be deployed in? How can we scientifically describe a "capability" in a system such as "reasoning" in a way that it can be rigorously measured and inform policy and deployment decisions? Recently he has pursued these questions by effectively "debunking" low quality evaluations and benchmarks of AI systems that serve more as marketing materials than measures of scientific progress, and in developing tools and practices to understand how multimodal systems can effectively handle culturally and linguistically diverse communities. He is graduating with a PhD in Computer Science from the University of California, Santa Barbara in June 2025 and will be joining the Tech Policy Lab at the University of Washington in the fall. His research has been covered in a variety of media outlets including IEEE Spectrum and TechCrunch, and he has been involved in AI policy discussions in the state of California, organizing an open letter from California academics with researcher and faculty signatories across all UC campuses, USC, Caltech, and Stanford, which helped inform Governor Newsom's decision to veto a scientifically unfounded "AI safety" bill.
