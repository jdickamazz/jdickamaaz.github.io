<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>A Pelican Blog - Liangming Pan</title><link href="/" rel="alternate"></link><link href="/feeds/liangming-pan.atom.xml" rel="self"></link><id>/</id><updated>2021-08-01T17:30:00+00:00</updated><entry><title>Zero-shot Fact Verification by Claim Generation</title><link href="/zero-shot-fact-verification-by-claim-generation.html" rel="alternate"></link><published>2021-08-01T17:30:00+00:00</published><updated>2021-08-01T17:30:00+00:00</updated><author><name>Liangming Pan</name></author><id>tag:None,2021-08-01:/zero-shot-fact-verification-by-claim-generation.html</id><summary type="html">&lt;p&gt;ACL-IJCNLP 2021 Paper "Zero-shot Fact Verification by Claim Generation"&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Fact verification&lt;/strong&gt; aims to validate a claim in the context of evidence. Given a claim $\mathcal{C}$ and a piece of evidence $\mathcal{P}$ as inputs, a fact verification model $\mathcal{F}$ predicts a label $\mathcal{Y} \in {\texttt{supported}, \texttt{refuted}, \texttt{NEI} }$ to verify whether $\mathcal{C}$ is supported, refuted, or can not be verified by the information in $\mathcal{P}$. Fact verification task has attracted growing interest with the rise in disinformation in news and social media. Rapid progress has been made by training large neural models on the &lt;a href="https://arxiv.org/abs/1803.05355"&gt;FEVER dataset&lt;/a&gt;, containing more than 100K human-crafted (evidence, claim) pairs based on Wikipedia.&lt;/p&gt;
&lt;p&gt;Fact verification is demanded in many domains, including news articles, social media, and scientific documents. However, it is not realistic to assume that large-scale training data is available for every new domain that requires fact verification. Creating training data by asking humans to write claims and search for evidence to support/refute them can be extremely costly. &lt;/p&gt;
&lt;h4&gt;Zero/Few-shot Fact Verification&lt;/h4&gt;
&lt;p&gt;Could we train a good fact checking model without human annotation? &lt;strong&gt;We explore this possibility of automatically generating large-scale (evidence, claim, label) data to train the fact verification model.&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;As illustrated in Figure 1 below, we propose a simple yet general framework &lt;strong&gt;Question Answering for Claim Generation (QACG)&lt;/strong&gt; to generate three types of claims from any given evidence: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Claims that are supported by the evidence&lt;/li&gt;
&lt;li&gt;Claims that are refuted by the evidence&lt;/li&gt;
&lt;li&gt;Claims that the evidence does Not have Enough Information (NEI) to verify&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Zero-shot Fact Verification&lt;/strong&gt;: We assume no human-annotated training example is available. We only use the generated (evidence, claim, label) data are used to train the fact verification model. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Few-shot Fact Verification&lt;/strong&gt;: We assume only a few human-labeled (evidence, claim) pairs are available. We first train the fact verification model with the generated data. Then we fine-tune the model with the limited amount of human-labeled data. &lt;/p&gt;
&lt;p&gt;!Fig!8!
&lt;img alt="zero-shot-fact-verification" class="img-fluid" src="images/2108/zero-shot-fact-verification.png"&gt;
Our zero-shot fact verification framework.&lt;/p&gt;
&lt;h4&gt;Main Observations&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Reducing Human Labor&lt;/strong&gt;: pretraining the fact verification model with generated claims greatly reduces the demand for in-domain human annotation. 
- &lt;strong&gt;Zero-shot Setting&lt;/strong&gt;: Although we do not use any human-labeled training examples, the model achieves over 70% of the F1 performance of a fully-supervised setting. 
- &lt;strong&gt;Few-shot Setting&lt;/strong&gt;: By fine-tuning the model with only 100 labeled examples, we further close the performance gap, achieving 89.1% of fully-supervised performance. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Improving Robustness&lt;/strong&gt;: When evaluating the model on an unbiased test set for FEVER, we find that training with generated claims also produces a more robust fact verification model. &lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Claim Generation Model&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;As illustrated in Figure 2 below, our claim generation model QACG has two major components: &lt;/p&gt;
&lt;h4&gt;Question Generator&lt;/h4&gt;
&lt;p&gt;A &lt;strong&gt;Question Generator&lt;/strong&gt; $\mathcal{G}$ takes as input an evidence $P$ and a text span $A$ from the given evidence and aims to generate a question $Q$ with $A$ as the answer. We implement this with the &lt;a href="https://arxiv.org/abs/1910.13461"&gt;BART model&lt;/a&gt;, a large transformer-based sequence-to-sequence model pretrained on 160GB of text. The model is finetuned on the SQuAD dataset processed by &lt;a href="https://arxiv.org/abs/1704.01792"&gt;Zhou et al. (2017)&lt;/a&gt;, where the model encodes the concatenation of the SQuAD passage and the answer text and then learns to decode the question. &lt;/p&gt;
&lt;h4&gt;QA-to-Claim model&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;QA-to-Claim model&lt;/strong&gt; $\mathcal{M}$ takes as inputs $Q$ and $A$, and outputs the declarative sentence $C$ for the $(Q, A)$ pair. We also treat this as a sequence-to-sequence problem and finetune the BART model on the &lt;a href="https://arxiv.org/abs/1809.02922"&gt;QA2D dataset&lt;/a&gt;, which contains the human-annotated declarative sentence for each $(Q, A)$ pair in SQuAD. &lt;/p&gt;
&lt;p&gt;!Fig!10!
&lt;img alt="framework" src="images/2108/framework.png"&gt;
Our claim generation model.&lt;/p&gt;
&lt;h3&gt;Using the Model to Generate Claims&lt;/h3&gt;
&lt;p&gt;Given the pretrained question generator $G$ and the QA-to-Claim model $M$, we then formally introduce how we generate claims with different labels. &lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Supported&lt;/em&gt; Claim Generation&lt;/h4&gt;
&lt;p&gt;Given an evidence $P$, we use named entity recognition to identify all entities within $P$. We treat each identified entity $a$ as an answer and generate a question $q$ with the question generator. The question–answer pair $(q, a)$ are then sent to the QA-to-Claim model to generate the supported claim $c$. &lt;/p&gt;
&lt;h4&gt;&lt;em&gt;Refuted&lt;/em&gt; Claim Generation&lt;/h4&gt;
&lt;p&gt;To generate a refuted claim, after we generate the question–answer pair $(q, a)$, we use &lt;strong&gt;answer replacement&lt;/strong&gt; to replace the answer $a$ with another entity $a'$ with the same type such that $a'$ becomes an incorrect answer to the question $q$. Using $a$ as the query, we randomly sample a phrase from the top-5 most similar phrases in the pretrained &lt;a href="https://github.com/explosion/sense2vec"&gt;Sense2Vec&lt;/a&gt; as the replacing answer $a'$. The new pair $(q, a')$ is then fed to the QA-to-Claim model to generate the refuted claim. &lt;/p&gt;
&lt;h4&gt;&lt;em&gt;NEI&lt;/em&gt; Claim Generation&lt;/h4&gt;
&lt;p&gt;We need to generate a question $q'$ which is relevant but cannot be answered by $P$. To this end, we link $P$ back to its original Wikipedia article $W$ and expand the evidence with additional contexts $P_{ext}$, which are five randomly-retrieved sentences from $W$ that are not present in $P$. As shown in the example in the above Figure, one additional context retrieved is "By the time the riots ended, 63 people had been killed". We then concatenate $P$ and $P_{ext}$ as the expanded evidence, based on which we generate a supported claim given an entity in $P_{ext}$ as the answer (e.g., "63"). This results in a claim relevant to but unverifiable by the original evidence $P$.  &lt;/p&gt;
&lt;h3&gt;Experiments&lt;/h3&gt;
&lt;p&gt;By applying our QACG model to each of the 18,541 Wikipedia articles in the FEVER training set, we generate a total number of 176,370 supported claims, 360,924 refuted claims, and 258,452 NEI claims. &lt;/p&gt;
&lt;h4&gt;Evaluation Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FEVER-S/R&lt;/strong&gt;: Since only the supported and refuted claims are labeled with gold evidence in FEVER, we take the claim–evidence pairs of these two classes from the FEVER test set for evaluation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FEVER-Symmetric&lt;/strong&gt;: this is a carefully-designed unbiased test set designed by &lt;a href="https://arxiv.org/abs/1908.05267"&gt;Schuster et al. (2019)&lt;/a&gt; to detect the robustness of the fact verification model. Only supported and refuted claims are present in this test set. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FEVER-S/R/N&lt;/strong&gt;: The full FEVER test set are used for a three-class verification. We use the system of &lt;a href="https://arxiv.org/abs/1901.02534"&gt;Malon (2019)&lt;/a&gt; to retrieve evidence sentences for NEI claims. &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Zero-shot Fact Verification&lt;/h4&gt;
&lt;p&gt;Models for comparison: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Supervised&lt;/strong&gt;: the RoBERTa-large model fine-tuned on the FEVER training set as the supervised model. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;QACG&lt;/strong&gt;: the RoBERTa-large model fine-tuned on our generated training set. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Perplexity&lt;/strong&gt;: It predicts the class label based on the perplexity of the claim under a pretrained GPT2 language model, following the assumption that "misinformation has high perplexity". &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LM as Fact Checker&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2006.04102"&gt;(Lee et al., 2020b)&lt;/a&gt;: It leverages the implicit knowledge stored in the pretrained BERT language model to verify a claim. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;!Fig!8!
&lt;img alt="zero-shot-experiment" src="images/2108/zero-shot-experiment.png"&gt;
Our zero-shot experiment.&lt;/p&gt;
&lt;p&gt;QACG attains 78.1 F1 on the FEVER-S/R and 62.6 F1 on the FEVER-S/R/N. Although QACG does not see any human-labelled (evidence, claim) pair in training, the F1 gap to the fully-supervised model is only 17.0 and 15.2 on these two settings, respectively. &lt;strong&gt;The results demonstrate the effectiveness of QACG in generating good (evidence, claim) pairs for training the fact verification model.&lt;/strong&gt; We observe a large performance drop when the supervised model is evaluated on the FEVER-Symmetric test set (−9.6 F1). However, the models trained with our generated data drop only 1.2 and 1.0 F1 drop. This suggests that the wide range of different claims we generate as training data helps eliminate some of the annotation artifacts present in FEVER, leading to a more robust fact verification model. &lt;/p&gt;
&lt;h4&gt;Few-shot Fact Verification&lt;/h4&gt;
&lt;p&gt;The blue solid line in the below Figure shows the F1 scores on FEVER-Symmetric after fine-tuning with different numbers of labeled training data. We compare this with training the model from scratch with the human-labeled data (grey dashed line). Our model performs consistently better than the model without pretraining, regardless of the amount of labeled training data. The improvement is especially prominent in data-poor regimes.  The results show pretraining fact verification with QACG greatly reduces the demand for in-domain humanannotated data. &lt;strong&gt;Our method can provide a "warm start" for fact verification system when applied to a new domain where training data are limited.&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;!Fig!5!
&lt;img alt="few-shot-fact-verification" src="images/2108/few-shot-fact-verification.png"&gt;
Our few-shot fact verification experiment.&lt;/p&gt;
&lt;h4&gt;Case Study&lt;/h4&gt;
&lt;p&gt;The below Table shows representative claims generated by our model. The claims are fluent, label-cohesive, and exhibit encouraging language variety.  However, one limitation is that our generated claims are mostly &lt;strong&gt;lack of deep reasoning over the evidence&lt;/strong&gt;. This is because we finetune the question generator on the SQuAD dataset, in which more than 80% of its questions are shallow factoid questions. &lt;/p&gt;
&lt;p&gt;!Fig!10!
&lt;img alt="example-claims" src="images/2108/example-claims.png"&gt;
Example claims generated by our model in the case study.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In this work, we utilize the question generation model to ask different questions for given evidence and convert question–answer pairs into claims with different labels. We show that the generated claims can train a well-performing fact verification model in both the zero-shot and the few-shot learning setting. In summary, our contributions are: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We explore the possibility of automatically &lt;strong&gt;generating large-scale (evidence, claim) pairs&lt;/strong&gt; to train the fact verification model.&lt;/li&gt;
&lt;li&gt;We propose a simple yet general framework &lt;strong&gt;Question Answering for Claim Generation (QACG)&lt;/strong&gt; to generate three types of claims from any given evidence: 1) claims that are &lt;strong&gt;supported&lt;/strong&gt; by the evidence, 2) claims that are &lt;strong&gt;refuted&lt;/strong&gt; by the evidence, and 3) claims that the evidence does &lt;strong&gt;Not have Enough Information (NEI)&lt;/strong&gt; to verify.&lt;/li&gt;
&lt;li&gt;We show that the generated training data can greatly benefit the fact verification system in both &lt;strong&gt;zero-shot and few-shot learning settings&lt;/strong&gt;. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can check out our full paper &lt;a href="https://arxiv.org/pdf/2105.14682.pdf"&gt;here&lt;/a&gt; and our source code/data on &lt;a href="https://github.com/teacherpeterpan/Zero-shot-Fact-Verification"&gt;GitHub&lt;/a&gt;. If you have questions, please feel free to email us.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Liangming Pan: &lt;a href="mailto:liangmingpan@u.nus.edu"&gt;liangmingpan@u.nus.edu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wenhu Chen: &lt;a href="mailto:wenhuchen@cs.ucsb.edu"&gt;wenhuchen@cs.ucsb.edu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong&gt;Acknowledgments&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;This blog post is based on the paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/2105.14682.pdf"&gt;Zero-shot Fact Verification by Claim Generation&lt;/a&gt;. &lt;a href="https://liangmingpan.com/"&gt;Liangming Pan&lt;/a&gt;, &lt;a href="https://wenhuchen.github.io/"&gt;Wenhu Chen&lt;/a&gt;, &lt;a href="https://xwhan.github.io/"&gt;Wenhan Xiong&lt;/a&gt;, &lt;a href="https://www.comp.nus.edu.sg/~kanmy/"&gt;Min-Yen Kan&lt;/a&gt;, and &lt;a href="https://sites.cs.ucsb.edu/~william/"&gt;William Yang Wang&lt;/a&gt;. ACL-IJCNLP 2021. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many thanks to my collaborators and advisors, Wenhu Chen, Wenhan Xiong, Min-Yen Kan and William Yang Wang for their help. Many thanks to Michael Saxon for edits on this blog post. &lt;/p&gt;</content><category term="ACL 2021"></category><category term="Fact Checking"></category><category term="Question Generation"></category><category term="Claim Generation"></category><category term="Zero-shot Learning"></category></entry></feed>